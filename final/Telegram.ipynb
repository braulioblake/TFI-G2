{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc56be46",
   "metadata": {},
   "source": [
    "# Workshop Final - NPL - Clasificación de Sentimientos\n",
    "\n",
    "**Integrantes del grupo:**\n",
    "\n",
    "- Kevin Tasat\n",
    "- Martha Alvarez\n",
    "- Fernando Roa\n",
    "\n",
    "### Resumen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecf5de",
   "metadata": {},
   "source": [
    "El dataset a evaluar tiene un total de 16000 textos a los cuales se les asocian diferentes sentimientos o emociones. El objetivo de este trabajo es mediante análisis de lenguaje natural preparar la información y posteriormente entrenar un modelo de clasificación de emociones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09908c80",
   "metadata": {},
   "source": [
    "## Telegram BOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56330c4e",
   "metadata": {},
   "source": [
    "### Cargue de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91da5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram.ext\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c6b874-3c3b-4355-acc9-74e94d0a58b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LinearSVC from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns;sns.set_theme(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.colors import ListedColormap\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report, auc\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from IPython.display import Image\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "#nltk.download('all')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from wordhoard import Antonyms\n",
    "import random\n",
    "\n",
    "from profanity_check import predict, predict_prob\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ff1c3a-658a-4f19-a17d-46477e428c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se construye un diccionario con contracciones y sus respectivas palabras extendidas\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71dd3d3-6a49-4e37-886f-b0a1050a2f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stop_words = pd.read_csv('stopwords.csv')\n",
    "stop_words = df_stop_words['stopwords'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8457be",
   "metadata": {},
   "source": [
    "### Creación de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277defa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clase para identificar negación antes de una palabra\n",
    "# transforma la negación en not y transforma la palabra en su antónimo\n",
    "# Lo anterior trata de invertir el significado luego de pasar el texto por \n",
    "# stop words\n",
    "\n",
    "class negation_transformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    # negate_sequence(text)\n",
    "    #   text: sentence to process (creation of uni/bi\n",
    "    #    is handled here)\n",
    "    #\n",
    "    # Detects negations and transforms negated words into 'not_' form\n",
    "    #\n",
    "    def negate_sequence(text):\n",
    "\n",
    "        def antonyms_for(word):\n",
    "            antonyms = set()\n",
    "            for ss in wn.synsets(word):\n",
    "                for lemma in ss.lemmas():\n",
    "                    any_pos_antonyms = [ antonym.name() for antonym in lemma.antonyms() ]\n",
    "                    for antonym in any_pos_antonyms:\n",
    "                        antonym_synsets = wn.synsets(antonym)\n",
    "                        if wn.ADJ not in [ ss.pos() for ss in antonym_synsets ]:\n",
    "                            continue\n",
    "                        antonyms.add(antonym)\n",
    "            if antonyms==set():\n",
    "                return word\n",
    "            else:\n",
    "                antonyms=list(antonyms)\n",
    "                return antonyms[0]\n",
    "\n",
    "        negation = False\n",
    "        delims = \"?.,!:;\"\n",
    "        result = []\n",
    "        words = text.split()\n",
    "        prev = None\n",
    "        pprev = None\n",
    "        for word in words:\n",
    "            stripped = word.strip(delims).lower()\n",
    "            negated = \"not \" + stripped if negation else stripped\n",
    "            result.append(negated)\n",
    "\n",
    "            if prev:\n",
    "                bigram = prev + \" \" + negated\n",
    "                pprev = prev\n",
    "            prev = negated\n",
    "\n",
    "            if any(neg in word for neg in [\"not\", \"n't\", \"no\"]):\n",
    "                negation = not negation\n",
    "\n",
    "            if any(c in word for c in delims):\n",
    "                negation = False\n",
    "\n",
    "        for elements in result:\n",
    "            \n",
    "            element=word_tokenize(elements)\n",
    "            \n",
    "            element0=0\n",
    "            element_change=1\n",
    "            \n",
    "            if element[0]==\"not\" and len(element)>1 and element[1]!=\"feel\":\n",
    "                \n",
    "                element0=\" \".join(element)\n",
    "                \n",
    "                element_change=antonyms_for(element[1])\n",
    "        \n",
    "            result=[element_change if item == element0 else item for item in result]\n",
    "\n",
    "        return \" \".join(result)\n",
    "    \n",
    "    def fit(self, texto, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texto):\n",
    "        if isinstance(texto, pd.Series):\n",
    "            texto=texto.apply(lambda x: self.negate_sequence(x))\n",
    "        else:\n",
    "            texto=pd.Series(texto)\n",
    "            texto=texto.apply(lambda x: self.negate_sequence(x))\n",
    "            \n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658878eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para limpiar texto\n",
    "\n",
    "class clean_texto(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "\n",
    "        # Convert words to lower case\n",
    "        text = text.lower()\n",
    "\n",
    "        # Replace contractions with their longer forms \n",
    "        if True:\n",
    "            text = text.split()\n",
    "            new_text = []\n",
    "            for word in text:\n",
    "                if word in contractions:\n",
    "                    new_text.append(contractions[word])\n",
    "                else:\n",
    "                    new_text.append(word)\n",
    "            text = \" \".join(new_text)\n",
    "\n",
    "        # Format words and remove unwanted characters\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'&amp;', '', text) \n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "        # remove stop words\n",
    "        #if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = stop_words\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "        # spelling correction (tarda demasiado y no da buenos resultados)\n",
    "        #text = TextBlob(text).correct() \n",
    "        #text = text.__str__() # opcion 1 (tarda demasiado)\n",
    "        #text = '{}'.format(text) #opcion2 (tarda demasiado)\n",
    "        #text = str(TextBlob(text).correct()) #opcion3 (tarda demasiado)\n",
    "\n",
    "        #Lemmatize \n",
    "\n",
    "        #Defaultdict is a container like dictionaries present in the module collections. \n",
    "        #Defaultdict is a sub-class of the dictionary class that returns a dictionary-like object. \n",
    "        #The functionality of both dictionaries and defaultdict are almost same except for the fact that defaultdict never raises a KeyError. \n",
    "        #It provides a default value for the key that does not exists.\n",
    "        \n",
    "        tag_map = defaultdict(lambda : wn.NOUN)\n",
    "        tag_map['J'] = wn.ADJ\n",
    "        tag_map['V'] = wn.VERB\n",
    "        tag_map['R'] = wn.ADV\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "\n",
    "        new_text2=[]\n",
    "\n",
    "        for token, tag in pos_tag(tokens):\n",
    "            lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
    "            new_text2.append(lemma)\n",
    "            text=\" \".join(new_text2)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def fit(self, texto, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texto):\n",
    "        if isinstance(texto, pd.Series):\n",
    "            texto=texto.apply(lambda x: self.clean_text(x))\n",
    "        else:\n",
    "            texto=pd.Series(texto)\n",
    "            texto=texto.apply(lambda x: self.clean_text(x))\n",
    "            \n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ecb588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el modelo\n",
    "\n",
    "path_pipe = \"pipe_model.sav\"\n",
    "pipe = joblib.load(path_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d521e",
   "metadata": {},
   "source": [
    "### Cargue del bot de telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8ab8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lala.py','r') as f:\n",
    "    token=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141f2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater=telegram.ext.Updater(token,use_context=True)\n",
    "disp=updater.dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829d3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(update,context):\n",
    "    update.message.reply_text(\"Hello I'm SentiBot, I'm here for you, please tell me how you feel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "360d0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ayuda(update, context):\n",
    "     update.message.reply_text(\"This is a Sentiment Analyzer Bot created by Group 2 of DH Academy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a233b119-edfe-4f1c-8d5b-b0ae48af3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(update, context):\n",
    "     update.message.reply_text(\"This is a natural language processing machine learning built in Bot. It objective is to predict basic sentiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a094a71-0579-4022-a916-8d9c80e1534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(update,context):\n",
    "    print(f\"Update {update} caused error {context.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31393e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_message(update,context):  \n",
    "    \n",
    "    delims = \"?.,!:;\"\n",
    "\n",
    "    user_message=update.message.text.strip(delims).lower()\n",
    "\n",
    "    if user_message.isnumeric():\n",
    "\n",
    "        return update.message.reply_text(\"That's not a feeling\")\n",
    "    \n",
    "    elif user_message in [\"hello\",\"hi\",\"whatsup\",\"hola\",\"hey\",\"good afternoon\"\n",
    "                          ,\"good morning\", \"good evening\", \"morning\", \"evening\"\n",
    "                          , \"whatsup\",\"hi there\",\"hey there\",\"howdy\",\"sup\"]:\n",
    "        \n",
    "        return update.message.reply_text(\"Hi! Nice to meet you. I'm here for you, please tell me how you feel\")\n",
    "    \n",
    "    elif user_message in [\"who are you\",\"who are you?\"]:\n",
    "        return update.message.reply_text(\"I'm SentiBot\")\n",
    "    \n",
    "    elif user_message in [\"goodbye\",\"bye\",\"adios\",\"chau\"]:\n",
    "        return update.message.reply_text(\"Goodbye, it was nice talking to you!\")\n",
    "    \n",
    "    elif user_message in [\"thanks\",\"thank you\",\"thank you very much\"]:\n",
    "        return update.message.reply_text(\"You are welcome. I hope I helped you\")\n",
    "    \n",
    "    elif predict([str(user_message)])[0]==1:\n",
    "        return update.message.reply_text(\"That's not very nice! let's try it again\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        prediction= pipe.predict(user_message)[0]\n",
    "\n",
    "        if prediction == \"anger\":\n",
    "\n",
    "            return update.message.reply_text(f'I detect {prediction} , remember \"Anger makes you smaller, while forgiveness forces you to grow beyond what you were.\" - Chérie Carter-Scott.')\n",
    "\n",
    "        elif prediction == \"fear\":\n",
    "\n",
    "            return update.message.reply_text(f'I detect {prediction} , remember \"Fear is the path to the dark side. Fear leads to anger. Anger leads to hate. Hate leads to suffering.\"- Yoda, The Phantom Menace.')\n",
    "\n",
    "        elif prediction == \"love\":\n",
    "\n",
    "            return update.message.reply_text(f'I detect {prediction} , remember \"Love is a fruit in season at all times, and within reach of every hand.\" - Mother Teresa')\n",
    "\n",
    "        elif prediction == \"joy\":\n",
    "\n",
    "            return update.message.reply_text(f'I detect {prediction} , remember \"A thing of beauty is a joy forever.\" ― John Keats, Endymion: A Poetic Romance')\n",
    "\n",
    "        elif prediction == \"surprise\":\n",
    "\n",
    "            return update.message.reply_text(f'I detect {prediction} , remember \"Surprise is the greatest gift which life can grant us.\" - Boris Pasternak')\n",
    "\n",
    "        elif prediction == \"sadness\":\n",
    "\n",
    "            return update.message.reply_text(f'I detect {prediction} , remember \"We must understand that sadness is an ocean, and sometimes we drown, while other days we are forced to swim.\" - R.M. Drake')\n",
    "\n",
    "        else:\n",
    "\n",
    "            return update.message.reply_text(f'I detect {prediction} but I don''t know')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c28d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.add_handler(telegram.ext.CommandHandler(\"start\",start))\n",
    "disp.add_handler(telegram.ext.CommandHandler(\"help\",ayuda))\n",
    "disp.add_handler(telegram.ext.CommandHandler(\"context\",context))\n",
    "disp.add_handler(telegram.ext.MessageHandler(telegram.ext.Filters.text,handle_message))\n",
    "disp.add_error_handler(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b78adad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater.start_polling()\n",
    "updater.idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce333fe3-efa8-4a3b-b087-0341b3b9b7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
