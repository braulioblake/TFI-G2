{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc56be46",
   "metadata": {},
   "source": [
    "# Workshop Final - NPL - Clasificación de Sentimientos\n",
    "\n",
    "**Integrantes del grupo:**\n",
    "\n",
    "- Kevin Tasat\n",
    "- Martha Alvarez\n",
    "- Fernando Roa\n",
    "\n",
    "### Resumen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecf5de",
   "metadata": {},
   "source": [
    "El dataset a evaluar tiene un total de 16000 textos a los cuales se les asocian diferentes sentimientos o emociones. El objetivo de este trabajo es mediante análisis de lenguaje natural preparar la información y posteriormente entrenar un modelo de clasificación de emociones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09908c80",
   "metadata": {},
   "source": [
    "## Telegram BOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56330c4e",
   "metadata": {},
   "source": [
    "### Cargue de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91da5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram.ext\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8457be",
   "metadata": {},
   "source": [
    "### Creación de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "277defa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clase para identificar negación antes de una palabra\n",
    "# transforma la negación en not y transforma la palabra en su antónimo\n",
    "# Lo anterior trata de invertir el significado luego de pasar el texto por \n",
    "# stop words\n",
    "\n",
    "class negation_transformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    # negate_sequence(text)\n",
    "    #   text: sentence to process (creation of uni/bi\n",
    "    #    is handled here)\n",
    "    #\n",
    "    # Detects negations and transforms negated words into 'not_' form\n",
    "    #\n",
    "    def negate_sequence(text):\n",
    "\n",
    "        def antonyms_for(word):\n",
    "            antonyms = set()\n",
    "            for ss in wn.synsets(word):\n",
    "                for lemma in ss.lemmas():\n",
    "                    any_pos_antonyms = [ antonym.name() for antonym in lemma.antonyms() ]\n",
    "                    for antonym in any_pos_antonyms:\n",
    "                        antonym_synsets = wn.synsets(antonym)\n",
    "                        if wn.ADJ not in [ ss.pos() for ss in antonym_synsets ]:\n",
    "                            continue\n",
    "                        antonyms.add(antonym)\n",
    "            if antonyms==set():\n",
    "                return word\n",
    "            else:\n",
    "                antonyms=list(antonyms)\n",
    "                return antonyms[0]\n",
    "\n",
    "        negation = False\n",
    "        delims = \"?.,!:;\"\n",
    "        result = []\n",
    "        words = text.split()\n",
    "        prev = None\n",
    "        pprev = None\n",
    "        for word in words:\n",
    "            stripped = word.strip(delims).lower()\n",
    "            negated = \"not \" + stripped if negation else stripped\n",
    "            result.append(negated)\n",
    "\n",
    "            if prev:\n",
    "                bigram = prev + \" \" + negated\n",
    "                pprev = prev\n",
    "            prev = negated\n",
    "\n",
    "            if any(neg in word for neg in [\"not\", \"n't\", \"no\"]):\n",
    "                negation = not negation\n",
    "\n",
    "            if any(c in word for c in delims):\n",
    "                negation = False\n",
    "\n",
    "        for elements in result:\n",
    "            \n",
    "            element=word_tokenize(elements)\n",
    "            \n",
    "            element0=0\n",
    "            element_change=1\n",
    "            \n",
    "            if element[0]==\"not\" and len(element)>1 and element[1]!=\"feel\":\n",
    "                \n",
    "                element0=\" \".join(element)\n",
    "                \n",
    "                element_change=antonyms_for(element[1])\n",
    "        \n",
    "            result=[element_change if item == element0 else item for item in result]\n",
    "\n",
    "        return \" \".join(result)\n",
    "    \n",
    "    def fit(self, texto, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texto):\n",
    "        if isinstance(texto, pd.Series):\n",
    "            texto=texto.apply(lambda x: self.negate_sequence(x))\n",
    "        else:\n",
    "            texto=pd.Series(texto)\n",
    "            texto=texto.apply(lambda x: self.negate_sequence(x))\n",
    "            \n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658878eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para limpiar texto\n",
    "\n",
    "class clean_texto(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "\n",
    "        # Convert words to lower case\n",
    "        text = text.lower()\n",
    "\n",
    "        # Replace contractions with their longer forms \n",
    "        if True:\n",
    "            text = text.split()\n",
    "            new_text = []\n",
    "            for word in text:\n",
    "                if word in contractions:\n",
    "                    new_text.append(contractions[word])\n",
    "                else:\n",
    "                    new_text.append(word)\n",
    "            text = \" \".join(new_text)\n",
    "\n",
    "        # Format words and remove unwanted characters\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'&amp;', '', text) \n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "        # remove stop words\n",
    "        #if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = stop_words\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "        # spelling correction (tarda demasiado y no da buenos resultados)\n",
    "        #text = TextBlob(text).correct() \n",
    "        #text = text.__str__() # opcion 1 (tarda demasiado)\n",
    "        #text = '{}'.format(text) #opcion2 (tarda demasiado)\n",
    "        #text = str(TextBlob(text).correct()) #opcion3 (tarda demasiado)\n",
    "\n",
    "        #Lemmatize \n",
    "\n",
    "        #Defaultdict is a container like dictionaries present in the module collections. \n",
    "        #Defaultdict is a sub-class of the dictionary class that returns a dictionary-like object. \n",
    "        #The functionality of both dictionaries and defaultdict are almost same except for the fact that defaultdict never raises a KeyError. \n",
    "        #It provides a default value for the key that does not exists.\n",
    "        \n",
    "        tag_map = defaultdict(lambda : wn.NOUN)\n",
    "        tag_map['J'] = wn.ADJ\n",
    "        tag_map['V'] = wn.VERB\n",
    "        tag_map['R'] = wn.ADV\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "\n",
    "        new_text2=[]\n",
    "\n",
    "        for token, tag in pos_tag(tokens):\n",
    "            lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
    "            new_text2.append(lemma)\n",
    "            text=\" \".join(new_text2)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def fit(self, texto, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texto):\n",
    "        if isinstance(texto, pd.Series):\n",
    "            texto=texto.apply(lambda x: self.clean_text(x))\n",
    "        else:\n",
    "            texto=pd.Series(texto)\n",
    "            texto=texto.apply(lambda x: self.clean_text(x))\n",
    "            \n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ecb588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo\n",
    "\n",
    "path_pipe = \"pipe_model.sav\"\n",
    "pipe = joblib.load(path_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d521e",
   "metadata": {},
   "source": [
    "### Cargue del bot de telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f8ab8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lala.py','r') as f:\n",
    "    token=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141f2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater=telegram.ext.Updater(token,use_context=True)\n",
    "disp=updater.dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829d3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(update,context):\n",
    "    update.message.reply_text(\"Hello I'm SentiBot, I'm here for you, please tell me how you feel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360d0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ayuda(update, context):\n",
    "     update.message.reply_text(\"This is a Sentiment Analyzer Bot created by Group 2 of DH Academy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31393e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_message(update,context):   \n",
    "    user_message=update.message.text\n",
    "    if user_message == \"111\":\n",
    "        return update.message.reply_text(\"That's not a feeling\")\n",
    "    else:\n",
    "        prediction= pipe.predict(user_message)[0]\n",
    "        return update.message.reply_text(f\"You feel {prediction}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c28d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.add_handler(telegram.ext.CommandHandler(\"start\",start))\n",
    "disp.add_handler(telegram.ext.CommandHandler(\"help\",ayuda))\n",
    "disp.add_handler(telegram.ext.MessageHandler(telegram.ext.Filters.text,handle_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78adad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater.start_polling()\n",
    "updater.idle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
